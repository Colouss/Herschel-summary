{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e58683-b949-4e26-82c5-271f1f4416d0",
   "metadata": {},
   "source": [
    "The objective of this notebook is to gather available data from all of the tables with photometry data from FUV to WISE, combine them with the measured Herschel data, and input everything in a .txt file that's labelled according to CIGALE's specifications.\n",
    "\n",
    "The first 2 code blocks is used to import all the necessary packages to efficiently run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95dda078-3860-4bd2-9914-71afcdbff288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary packages\n",
    "\n",
    "from astropy.table import Table\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "from astropy.io.ascii import masked\n",
    "from astropy.io import ascii\n",
    "import glob\n",
    "from astropy.io import fits\n",
    "import wget\n",
    "import matplotlib.image as mpimg\n",
    "from astropy.wcs import WCS\n",
    "from scipy.stats import scoreatpercentile\n",
    "from astropy.visualization import simple_norm\n",
    "from reproject import reproject_interp\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "from photutils.detection import DAOStarFinder\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from photutils.aperture import CircularAperture\n",
    "from astropy.visualization import SqrtStretch\n",
    "from astropy.visualization import ImageNormalize\n",
    "from astropy.visualization import LogStretch\n",
    "from astropy.wcs import WCS\n",
    "import glob\n",
    "from scipy.stats import scoreatpercentile\n",
    "import astropy.units as u\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import colors\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# Setup directories (adjust to your environment)\n",
    "os.environ['HOME'] ='C:/Users/USER/Documents/GitHub'\n",
    "homedir = os.getenv(\"HOME\")\n",
    "tabledir = homedir+'/Herschel summary/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa064e4-9d15-4181-ad08-da0ed1462e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CIGALEInputprep import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419ad8c6-2928-47cf-8bcf-3ad4361277e2",
   "metadata": {},
   "source": [
    "First thing we'll do is to convert the photometry table's inverse variance into uncertainty. The function works by looking for any columns that starts with \"FLUX_IVAR_AP06\" in it, and transform it by doing an inverse square of that value. In order to make it work for all 6 apertures though, we will need to change that code so that it will iterate through all 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf55689-cc29-4db4-85e8-2dbc48058830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully written to outputuncert.csv\n"
     ]
    }
   ],
   "source": [
    "#First, we need to convert the table's inverse variance into uncertainty\n",
    "input_file = tabledir+'Photometrytesting2.csv'\n",
    "output_file = 'outputuncert.csv' \n",
    "\n",
    "inverse_variance_into_uncertainty(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e79644-2470-4424-a25b-f08d3bee5f1a",
   "metadata": {},
   "source": [
    "Once we have the uncertainties of these measurements, we need to add extinction data from the VFS into it using the extinction table fits. And we can do this by finding the appropriate extinction columns, and multiplying it by the appropriate wavelengths insde the photometry tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f321cb77-6ee0-4eaf-b1e4-e513d506c572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result successfully written to extinctoutput.csv\n"
     ]
    }
   ],
   "source": [
    "# Then we add extinction into the table measurements. The extinction data is from the extinction table fits, trimmed to only have Herschel galaxies.\n",
    "file1 = tabledir + 'outputuncert.csv'\n",
    "file2 = tabledir + 'trimmedextinction.csv'\n",
    "\n",
    "# List of column pairs to multiply (from file1, file2)\n",
    "column_pairs = [\n",
    "    ('FLUX_AP06_G', 'A(G)_SFD'),\n",
    "    ('FLUX_UNCERT_AP06_G', 'A(G)_SFD'),\n",
    "    ('FLUX_AP06_R', 'A(R)_SFD'),\n",
    "    ('FLUX_UNCERT_AP06_R', 'A(Z)_SFD'), \n",
    "    ('FLUX_AP06_Z', 'A(Z)_SFD'),\n",
    "    ('FLUX_UNCERT_AP06_Z', 'A(Z)_SFD'),\n",
    "    ('FLUX_AP06_FUV', 'A(FUV)_SFD'),\n",
    "    ('FLUX_UNCERT_AP06_FUV', 'A(FUV)_SFD'),\n",
    "    ('FLUX_AP06_NUV', 'A(NUV)_SFD'),\n",
    "    ('FLUX_UNCERT_AP06_NUV', 'A(NUV)_SFD'),\n",
    "    ('FLUX_AP06_W1', 'A(W1)_SFD'),\n",
    "    ('FLUX_UNCERT_AP06_W1', 'A(W1)_SFD'),\n",
    "    ('FLUX_AP06_W2', 'A(W2)_SFD'),\n",
    "    ('FLUX_UNCERT_AP06_W2', 'A(W2)_SFD'),\n",
    "    ('FLUX_AP06_W3', 'A(W3)_SFD'),\n",
    "    ('FLUX_UNCERT_AP06_W3', 'A(W3)_SFD'),\n",
    "    ('FLUX_AP06_W4', 'A(W4)_SFD'),\n",
    "    ('FLUX_UNCERT_AP06_W4', 'A(W4)_SFD'),\n",
    "]\n",
    "\n",
    "output_file = 'extinctoutput.csv'  # Name of the output file\n",
    "\n",
    "multiply_columns_and_save(file1, file2, column_pairs, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4156b28-52ee-4812-93ee-ceb0c41475d3",
   "metadata": {},
   "source": [
    "Because the photometry table still has a lot of information that's not necessary for CIGALE inputs, I then remove any columns that I won't be using in order to make everything easier to keep track of. In addition to this, I also convert all of the Fluxes (that are not Herschel's) from nanomaggies to micro Janskies, as that is what CIGALE takes in. The factor used is after consulting with Kim about it. As for the unit conversions on Herschel galaxies, I instead multiply it by 1000 to go from Janskies to micro Janskies. Then, I create error columns for the Herschel fluxes by creating a column and multiplying every value by 5%. This number can be further finetuned. Then, we do a few more checks such as a error limit cut, as well as converting negative fluxes to blanks, as well as rounding the inputs so that they can be compatible with CIGALE. Then we output only what we need into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c358d419-ae20-4241-813e-b192809fc30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the ephots table to only the columns I need\n",
    "csv_input_file = tabledir + 'extinctoutput.csv'\n",
    "csv_output_file = 'finalreal.csv' \n",
    "\n",
    "# Specify the columns you want to output\n",
    "columns_to_keep = ['VF_ID', 'GALAXY','RA_MOMENT','DEC_MOMENT', 'FLUX_AP06_G', 'FLUX_UNCERT_AP06_G', 'FLUX_AP06_R', 'FLUX_UNCERT_AP06_R','FLUX_AP06_Z', 'FLUX_UNCERT_AP06_Z','FLUX_AP06_FUV', 'FLUX_UNCERT_AP06_FUV','FLUX_AP06_NUV', 'FLUX_UNCERT_AP06_NUV',\n",
    "                   'FLUX_AP06_W1', 'FLUX_UNCERT_AP06_W1','FLUX_AP06_W2', 'FLUX_UNCERT_AP06_W2','FLUX_AP06_W3', 'FLUX_UNCERT_AP06_W3','FLUX_AP06_W4', 'FLUX_UNCERT_AP06_W4'\n",
    "                  ,'70Flux_AP06','100Flux_AP06','160Flux_AP06'] \n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_input_file)\n",
    "\n",
    "# Format the VF_ID column to 'VFIDxxxx' with leading zeros\n",
    "df['VF_ID'] = df['VF_ID'].apply(lambda x: f'VFID{x:04d}')\n",
    "\n",
    "# Multiply all columns that start with 'FLUX' by 3.631e-3 as the conversion for nanomaggies to Jansky\n",
    "factor = 3.631e-3\n",
    "flux_columns = [col for col in df.columns if col.startswith('FLUX')]\n",
    "df[flux_columns] = df[flux_columns] * factor\n",
    "\n",
    "# Multiply specific columns by 1000 before any further calculations to convert to Jansky for Herschel\n",
    "df[['70Flux_AP06', '100Flux_AP06', '160Flux_AP06']] = df[['70Flux_AP06', '100Flux_AP06', '160Flux_AP06']] * 1000\n",
    "\n",
    "# Create error columns by multiplying the original flux columns by 5%\n",
    "error_cols = {\n",
    "    '70Flux_AP06_err': df['70Flux_AP06'] * 0.05,\n",
    "    '100Flux_AP06_err': df['100Flux_AP06'] * 0.05,\n",
    "    '160Flux_AP06_err': df['160Flux_AP06'] * 0.05\n",
    "}\n",
    "\n",
    "# Add the new error columns using pd.concat to avoid fragmentation\n",
    "df = pd.concat([df, pd.DataFrame(error_cols)], axis=1)\n",
    "\n",
    "# Add the new error columns to the list of columns\n",
    "columns_to_keep.extend(['70Flux_AP06_err', '100Flux_AP06_err', '160Flux_AP06_err'])\n",
    "\n",
    "# Iterate over each pair of FLUX and FLUX_UNCERT columns and apply the check\n",
    "for flux_col in flux_columns:\n",
    "    uncert_col = flux_col.replace('FLUX_', 'FLUX_UNCERT_')\n",
    "    if uncert_col in df.columns:\n",
    "        # Replace the value in the FLUX_UNCERT column if it's smaller than 5% of the corresponding FLUX column, this is to keep the errors to a minimum of 5% to keep results somewhat accurate\n",
    "        df[uncert_col] = df[[flux_col, uncert_col]].apply(lambda row: max(row[uncert_col], 0.05 * row[flux_col]), axis=1)\n",
    "\n",
    "# Check for negative values and replace with NaN, this can be commented out if we want potentially negative fluxes in the CIGALE inputs\n",
    "columns_to_check = flux_columns + ['70Flux_AP06', '100Flux_AP06', '160Flux_AP06'] + [col for col in df.columns if col.endswith('_err')]\n",
    "#df[columns_to_check] = df[columns_to_check].map(lambda x: np.nan if x < 0 else x)\n",
    "\n",
    "# Round all numerical values to 3 decimal places\n",
    "df[columns_to_check] = df[columns_to_check].round(3)\n",
    "\n",
    "# Select only the specified columns\n",
    "df_selected = df[columns_to_keep].copy()\n",
    "\n",
    "# Rename the VF_ID column to VFID\n",
    "df_selected.rename(columns={'VF_ID': 'VFID'}, inplace=True)\n",
    "\n",
    "# Write the selected columns to a new CSV file\n",
    "df_selected.to_csv(csv_output_file, index=False)\n",
    "\n",
    "print(f\"Selected columns successfully written to {csv_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711652cb-b553-46be-97d9-0795d137efab",
   "metadata": {},
   "source": [
    "The final block is used to generate the two .txt files directly from the table generated above. The two files are separated using the north and south designation using the galaxies's DEC. We also need a redshift measurements of these galaxies, which is taken from the environments .fits table from the VFS. Once everything is read in, I then follow the template Kim gave to write out my two input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7e91dd-e9df-4fa6-871c-c95addf0c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out the flux data to a text file for CIGALE\n",
    "# Define paths for north and south output files\n",
    "north_path = homedir + '/Table-generating/vf_data_north.txt'\n",
    "south_path = homedir + '/Table-generating/vf_data_south.txt'\n",
    "\n",
    "# Read the main CSV file\n",
    "csv_input_file = homedir + '/Table-generating/finalreal.csv'\n",
    "flux_tab = pd.read_csv(csv_input_file)\n",
    "\n",
    "# Replace zeros with NaN\n",
    "flux_tab.replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Read the second CSV file containing the redshift information\n",
    "redshift_file = homedir + '/Table-generating/trimmedenvironment2.csv'\n",
    "redshift_tab = pd.read_csv(redshift_file)\n",
    "\n",
    "# Merge flux_tab with redshift_tab based on 'VFID'\n",
    "flux_tab = pd.merge(flux_tab, redshift_tab[['VFID', 'redshift']], on='VFID', how='left')\n",
    "\n",
    "# Create flags for north and south based on DEC_MOMENT\n",
    "north_flag = flux_tab['DEC_MOMENT'] > 32\n",
    "south_flag = flux_tab['DEC_MOMENT'] < 32\n",
    "\n",
    "# Function to check and create directories if they don't exist\n",
    "def check_dir(*paths):\n",
    "    for path in paths:\n",
    "        directory = os.path.dirname(path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    \n",
    "# Check and create directories for the output files\n",
    "check_dir(north_path, south_path)\n",
    "\n",
    "# Write north data\n",
    "with open(north_path, 'w') as file:\n",
    "    # Create file header\n",
    "    s = '# id redshift FUV FUV_err NUV NUV_err BASS-g BASS-g_err BASS-r BASS-r_err WISE1 WISE1_err WISE2 WISE2_err WISE3 WISE3_err WISE4 WISE4_err PACS_blue PACS_blue_err PACS_green PACS_green_err PACS_red PACS_red_err\\n'\n",
    "    file.write(s)\n",
    "    \n",
    "    # Write data rows\n",
    "    for _, n in flux_tab[north_flag].iterrows():\n",
    "        s_gal = f\"{n['VFID']} {n['redshift']} {n['FLUX_AP06_FUV']} {n['FLUX_UNCERT_AP06_FUV']} {n['FLUX_AP06_NUV']} {n['FLUX_UNCERT_AP06_NUV']} \" \\\n",
    "                f\"{n['FLUX_AP06_G']} {n['FLUX_UNCERT_AP06_G']} {n['FLUX_AP06_R']} {n['FLUX_UNCERT_AP06_R']} \" \\\n",
    "                f\"{n['FLUX_AP06_W1']} {n['FLUX_UNCERT_AP06_W1']} {n['FLUX_AP06_W2']} {n['FLUX_UNCERT_AP06_W2']} \" \\\n",
    "                f\"{n['FLUX_AP06_W3']} {n['FLUX_UNCERT_AP06_W3']} {n['FLUX_AP06_W4']} {n['FLUX_UNCERT_AP06_W4']} \" \\\n",
    "                f\"{n['70Flux_AP06']} {n['70Flux_AP06_err']} {n['100Flux_AP06']} {n['100Flux_AP06_err']} {n['160Flux_AP06']} {n['160Flux_AP06_err']}\\n\"\n",
    "        file.write(s_gal)\n",
    "\n",
    "# Write south data\n",
    "with open(south_path, 'w') as file:\n",
    "    # Create file header\n",
    "    s = '# id redshift FUV FUV_err NUV NUV_err decamDR1-g decamDR1-g_err decamDR1-r decamDR1-r_err decamDR1-z decamDR1-z_err WISE1 WISE1_err WISE2 WISE2_err WISE3 WISE3_err WISE4 WISE4_err PACS_blue PACS_blue_err PACS_green PACS_green_err PACS_red PACS_red_err\\n'\n",
    "    file.write(s)\n",
    "    \n",
    "    # Write data rows\n",
    "    for _, n in flux_tab[south_flag].iterrows():\n",
    "        s_gal = f\"{n['VFID']} {n['redshift']} {n['FLUX_AP06_FUV']} {n['FLUX_UNCERT_AP06_FUV']} {n['FLUX_AP06_NUV']} {n['FLUX_UNCERT_AP06_NUV']} \" \\\n",
    "                f\"{n['FLUX_AP06_G']} {n['FLUX_UNCERT_AP06_G']} {n['FLUX_AP06_R']} {n['FLUX_UNCERT_AP06_R']} {n['FLUX_AP06_Z']} {n['FLUX_UNCERT_AP06_Z']} \" \\\n",
    "                f\"{n['FLUX_AP06_W1']} {n['FLUX_UNCERT_AP06_W1']} {n['FLUX_AP06_W2']} {n['FLUX_UNCERT_AP06_W2']} \" \\\n",
    "                f\"{n['FLUX_AP06_W3']} {n['FLUX_UNCERT_AP06_W3']} {n['FLUX_AP06_W4']} {n['FLUX_UNCERT_AP06_W4']} \" \\\n",
    "                f\"{n['70Flux_AP06']} {n['70Flux_AP06_err']} {n['100Flux_AP06']} {n['100Flux_AP06_err']} {n['160Flux_AP06']} {n['160Flux_AP06_err']}\\n\"\n",
    "        file.write(s_gal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
